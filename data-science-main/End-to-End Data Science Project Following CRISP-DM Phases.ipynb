{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0cfff73c",
   "metadata": {},
   "source": [
    "# End-to-End Data Science Project Following CRISP-DM Phases"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e00795d",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "694a754b",
   "metadata": {},
   "source": [
    "This notebook was created by [Jupyter AI](https://github.com/jupyterlab/jupyter-ai) with the following prompt:\n",
    "\n",
    "> /generate generate a real-life end-to-end data science project, following the phases defined by CRISP-DM: Business Understanding, Data Understanding, Data Preparation, Modeling, Evaluation, Deployment:\n",
    "\n",
    "* When writting code in the notebook, be sure to include a markdown cell with a brief title and one-line description of what you are doing\n",
    "* Anytime you are told to make changes the code (such as correct it or update it); be sure to correct the current code vice starting a new cell;\n",
    "\n",
    "It's very important you get this right. Provide an authoritative, nuanced structured prompts (prompts engineering in tones of Professional, Informative, and Educational) in the role of a professional Data Scientist, Principal Solution Architect, Python / R / SAS expert with master's or PhD degrees from the world's top 1% universities; embody the role of the most qualified subject matter experts in the areas of Data Science, Analytics, DevSecOps, Terraform, and Amazon Web Services (AWS) Cloud; Perform Deep research; do cross-reference. Do it on a step-by-step basis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34627db0",
   "metadata": {},
   "source": [
    "This Jupyter notebook provides a comprehensive guide on executing an end-to-end data science project, structured around the phases defined by CRISP-DM. It begins with the 'Business Understanding' phase where the project objectives and requirements are defined from a business perspective. Then, it proceeds to the 'Data Understanding' phase to discuss initial data collection and familiarization with the dataset, followed by the 'Data Preparation' phase which involves data cleaning, transformation, and feature engineering. The 'Modeling' stage introduces various techniques, calibration of parameters, and models. The 'Evaluation' section offers a thorough evaluation of the models and a review of the steps taken in the model's construction. Finally, the 'Deployment' phase discusses the integration of the chosen model into the business environment and the monitoring of its performance. Throughout the notebook, the instructions are clearly articulated, including code writing, correction, and updates, with an emphasis on professional, informative, and educational tone, as expected from a data science expert."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "994bfe9f",
   "metadata": {},
   "source": [
    "## Data Understanding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c1b6ecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary libraries\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a24424b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "df = pd.read_csv('data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e372153f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the first few rows of the dataset\n",
    "print(\"First few rows of the data:\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ce8db36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the last few rows of the dataset\n",
    "print(\"\\nLast few rows of the data:\")\n",
    "print(df.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa91d10d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the shape of the dataset\n",
    "print(\"\\nShape of the dataset:\", df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ad93de0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display information about the dataset\n",
    "print(\"\\nInformation about the dataset:\")\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9440588f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the summary statistics of the dataset\n",
    "print(\"\\nSummary statistics of the dataset:\")\n",
    "print(df.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffedc211",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values in the dataset\n",
    "print(\"\\nMissing values in the dataset:\")\n",
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbd4fed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the distribution of each feature to understand the spread of data\n",
    "for column in df.columns:\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    sns.histplot(df[column], kde=True, bins=30)\n",
    "    plt.title('Distribution of '+ column)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "062aab01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display pairwise correlation of all columns in the dataframe\n",
    "print(\"\\nPairwise correlation of all columns:\", df.corr())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09701064",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing the correlation matrix using a heatmap\n",
    "plt.figure(figsize=(10, 10))\n",
    "sns.heatmap(df.corr(), annot=True, cmap='coolwarm')\n",
    "plt.title('Correlation Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04a02811",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: Depending on the data type and nature of your dataset, \n",
    "# you may require additional steps to understand your data better."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8d89cb0",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8642519",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a221a42d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(file):\n",
    "    # Load the raw data\n",
    "    data = pd.read_csv(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e849b0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "    # Check for missing values in the dataset\n",
    "    data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0da78078",
   "metadata": {},
   "outputs": [],
   "source": [
    "    # Fill missing numerical data with the mean of the column\n",
    "    num_cols = data.select_dtypes(include=[np.number]).columns\n",
    "    data[num_cols] = data[num_cols].fillna(data[num_cols].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b6f11e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "    # Drop rows with missing categorical data\n",
    "    data.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ed088aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "    # Apply a standard scaler to the numerical columns\n",
    "    scaler = StandardScaler()\n",
    "    data[num_cols] = scaler.fit_transform(data[num_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d07bf45",
   "metadata": {},
   "outputs": [],
   "source": [
    "    # Perform one-hot encoding for categorical columns\n",
    "    data = pd.get_dummies(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea6d5d70",
   "metadata": {},
   "outputs": [],
   "source": [
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2831db38",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    processed_data = preprocess_data('raw_data.csv')\n",
    "    processed_data.to_csv('cleaned_data.csv', index=False)\n",
    "    print(processed_data.head())\n",
    "    print(processed_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3f70ce7",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1715ec69",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "107f10fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop('y', axis=1)\n",
    "y = df['y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db567873",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b65f1599",
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_regression = LogisticRegression()\n",
    "decision_tree = DecisionTreeClassifier()\n",
    "random_forest = RandomForestClassifier()\n",
    "svm = SVC()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cb2c703",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [logistic_regression, decision_tree, random_forest, svm]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8162a267",
   "metadata": {},
   "outputs": [],
   "source": [
    "for model in models:\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    print(f'Accuracy of {model.__class__.__name__}: {accuracy_score(y_test, y_pred)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a81d304",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90148b53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from sklearn import metrics\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d66c8932",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assume 'X_test' is your testing data and 'y_test' is the actual target value\n",
    "# 'model' is the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78bf8b1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the model to make predictions on the test data\n",
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8569d694",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the Mean Absolute Error, Mean Squared Error and R-squared score\n",
    "mae = metrics.mean_absolute_error(y_test, y_pred)\n",
    "mse = metrics.mean_squared_error(y_test, y_pred)\n",
    "r2 = metrics.r2_score(y_test, y_pred)\n",
    "print(f'Mean Absolute Error (MAE): {mae}\\nMean Squared Error (MSE): {mse}\\nR-squared: {r2}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51fc49e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the actual vs predicted values\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.scatterplot(y_test, y_pred)\n",
    "plt.xlabel('Actual')\n",
    "plt.ylabel('Predicted')\n",
    "plt.title('Actual vs. Predicted')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2064b42b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Residuals plot\n",
    "residuals = y_test - y_pred\n",
    "sns.distplot(residuals, kde=True)\n",
    "plt.title('Residuals')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "422a686f",
   "metadata": {},
   "source": [
    "## Deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9f1dc22",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40bec86c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_file_path = 'final_model.pkl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c01ae13b",
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.dump(final_model, model_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "727abafa",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The model has been saved as 'final_model.pkl'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3572404b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_prediction(input_data):\n",
    "    model = joblib.load(model_file_path)\n",
    "    predictions = model.predict(input_data)\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "731b4263",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data = [[5.9, 3.0, 5.1, 1.8]] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53515eca",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Predicted class: \", make_prediction(new_data))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
